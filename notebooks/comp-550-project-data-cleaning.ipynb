{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 550 Project - Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Samy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import block\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('punkt') # link to documentation on punkt tokenizers: https://www.nltk.org/_modules/nltk/tokenize/punkt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Storage - 20 Newsgroup\n",
    "### This notebook assumes that you've downloaded the [20 newsgroups dataset](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) and unpacked the tarball in the same directory as this notebook. \n",
    "\n",
    "### You should therefore have a folder called \"20_newsgroup\" in the same directory as this notebook. The \"20_newsgroup\" folder should have 20 subfolders (\"comp.os.ms-windows.misc\", \"soc.religion.christian\", \"rec.sport.baseball\", etc...), each containing a long list of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Format - 20 Newsgroup\n",
    "### Fortunately, the files are all raw text. Unfortunately, their headers can vary quite a lot. Take the following examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample file #1 from alt.atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(os.getcwd(), \"20_newsgroups\", \"alt.atheism\", \"51128\"))) as atheism_sample_file_1:\n",
    "    print(''.join(atheism_sample_file_1.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample file #2 from alt.atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(os.getcwd(), \"20_newsgroups\", \"alt.atheism\", \"49960\"))) as atheism_sample_file_2:\n",
    "    print(''.join(atheism_sample_file_2.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample file #1 from alt.autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(os.getcwd(), \"20_newsgroups\", \"rec.autos\", \"101553\"))) as auto_sample_file_1:\n",
    "    print(''.join(auto_sample_file_1.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample file #2 from alt.autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(os.getcwd(), \"20_newsgroups\", \"rec.autos\", \"103338\"))) as auto_sample_file_2:\n",
    "    print(''.join(auto_sample_file_2.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Format (cont'd)\n",
    "### The four examples above each had a \"Subject\" line in their headers, and it might be useful to use the subsequent words in our analysis. I however believe that we should only do so if we can do it consistently (for every file in every newsgroup), so I checked that every file in the 20 newsgroups had a line starting with \"Subject:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examined a total of 19997 files out of 19997, 0 of which didn't have a subject line.\n"
     ]
    }
   ],
   "source": [
    "root_dir = '\\\\\\\\?\\\\' + os.path.abspath(os.path.join(os.getcwd(), '20_newsgroups'))\n",
    "file_count = 0\n",
    "missing_subject_line_count = 0\n",
    "total_files_count = 0\n",
    "subdirectory_list = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "for subdir in subdirectory_list:\n",
    "    files = [ os.path.join(root_dir, subdir, f) for f in os.listdir(os.path.join(root_dir, subdir)) if '.onetoc2' not in f ]\n",
    "    total_files_count += len(files)\n",
    "    for current_file in files:\n",
    "        with open(current_file,'r') as input_file:\n",
    "            contents = input_file.readlines()\n",
    "        found_subject_line = False\n",
    "        for line in contents:\n",
    "            if \"subject:\" in line.lower():\n",
    "                found_subject_line = True\n",
    "                break\n",
    "        if not found_subject_line:\n",
    "            print(f\"{current_file} didn't have a subject line.\")\n",
    "            missing_subject_line_count += 1\n",
    "        file_count += 1\n",
    "\n",
    "print(f\"examined a total of {file_count} files out of {total_files_count}, {missing_subject_line_count} of which didn't have a subject line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, so every file has a subject line that we can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Extraction\n",
    "### Some heuristics:\n",
    "1. The subject line for every file will be considered as valid content.\n",
    "2. All lines below the \"Lines: #\" line will be considered as valid content. Some invalid lines may be included (e.g. \"NNTP-Posting-Host: punisher.caltech.edu\", or \"sandvik@newton.apple.com (Kent Sandvik) writes:\"), but we can process those lines' contents afterwords with NLTK or another english dictionary API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### super_dictionary is a dictionary with subdirectory::dict() key::value pairs. The values start off as empty dictionaries, but they are eventually populated with filename::list(valid lines) key::value pairs. \n",
    "\n",
    "### In case that's not clear, super_dictionary is supposed to end up looking like a tree like this:\n",
    "\n",
    "    super_dictionary = {\n",
    "\n",
    "        'alt.atheism' : {\n",
    "\n",
    "                            '49960' : [ list of all the valid lines in file 49960 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                            '51060' : [ list of all the valid lines in file 51060 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                            ...\n",
    "\n",
    "                        },\n",
    "\n",
    "        'comp.graphics' : {\n",
    "\n",
    "                              '37261': [ list of all the valid lines in file 37261 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                              '37913': [ list of all the valid lines in file 37913 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                              ...\n",
    "\n",
    "                          } ,\n",
    "\n",
    "        ...\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_dictionary = {\n",
    "    'alt.atheism': dict([]), \n",
    "    'comp.graphics': dict([]), \n",
    "    'comp.os.ms-windows.misc': dict([]), \n",
    "    'comp.sys.ibm.pc.hardware': dict([]), \n",
    "    'comp.sys.mac.hardware': dict([]), \n",
    "    'comp.windows.x': dict([]), \n",
    "    'misc.forsale': dict([]), \n",
    "    'rec.autos': dict([]), \n",
    "    'rec.motorcycles': dict([]), \n",
    "    'rec.sport.baseball': dict([]), \n",
    "    'rec.sport.hockey': dict([]), \n",
    "    'sci.crypt': dict([]), \n",
    "    'sci.electronics': dict([]), \n",
    "    'sci.med': dict([]), \n",
    "    'sci.space': dict([]), \n",
    "    'soc.religion.christian': dict([]), \n",
    "    'talk.politics.guns': dict([]), \n",
    "    'talk.politics.mideast': dict([]), \n",
    "    'talk.politics.misc': dict([]), \n",
    "    'talk.religion.misc': dict([])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating super_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'super_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f22c8b642945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msubdirectory_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0msubdirectory_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines_with_valid_content\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'super_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "root_dir = '\\\\\\\\?\\\\' + os.path.abspath(os.path.join(os.getcwd(), '20_newsgroups'))\n",
    "file_count = 0\n",
    "subdirectory_list = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "for subdir in subdirectory_list:\n",
    "    files = [ os.path.join(root_dir, subdir, f) for f in os.listdir(os.path.join(root_dir, subdir)) if '.onetoc2' not in f ]\n",
    "    for current_file in files:\n",
    "        lines_with_valid_content = []\n",
    "        with open(current_file,'r') as input_file:\n",
    "            contents = input_file.readlines()\n",
    "\n",
    "        for i, line in enumerate(contents):\n",
    "            if \"lines: \" in line.lower():\n",
    "                lines_with_valid_content.append(line.rstrip())\n",
    "                break\n",
    "        \n",
    "        i += 1   \n",
    "        while i < len(contents):\n",
    "            lines_with_valid_content.append(contents[i].rstrip())\n",
    "            i += 1\n",
    "        \n",
    "        subdirectory_dictionary = super_dictionary[subdir]\n",
    "        subdirectory_dictionary[os.path.basename(current_file)] = lines_with_valid_content\n",
    "        \n",
    "        file_count += 1\n",
    "\n",
    "print(f\"examined a total of {file_count} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually validating that the super_dictionary's end-value lists have the correct content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_atheism_dictionary = super_dictionary[\"alt.atheism\"]\n",
    "print('\\n'.join(alt_atheism_dictionary[\"49960\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling super_dictionary into 20_newsgroup_content_dictionary.pickle in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20_newsgroup_content_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(super_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# just to show that pickling works\n",
    "#with open('20_newsgroup_content_dictionary.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)\n",
    "#\n",
    "#print(super_dictionary == b)\n",
    "#del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### For every leaf node (file) in the super_dictionary, we need to \"clean\" the leaf node's list of valid lines. \n",
    "### This can be done in a number of ways: here I've used NLTK's punkt tokenizer (https://www.nltk.org/_modules/nltk/tokenize/punkt.html), as it can tokenize with non-alphanumeric characters as well as with spaces. See the toy example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_some_alphanumeric_characters(line):\n",
    "    if re.search('[a-zA-Z]', line):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def tokenize(list_of_lines, verbose=False):\n",
    "    tokenized_list = []\n",
    "    for line in list_of_lines:\n",
    "        tokens = [word for word in nltk.word_tokenize(line) if has_some_alphanumeric_characters(word)]\n",
    "        if verbose:\n",
    "            print('The line \"{}\" becomes:\\n{}\\n\\n'.format(str(line), ', '.join(tokens)))\n",
    "        for tok in tokens:\n",
    "            tokenized_list.append(tok)\n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example_list_of_valid_lines = [\"NNTP-Posting-Host: punisher.caltech.edu\",\n",
    "                                   \"sandvik@newton.apple.com (Kent Sandvik) writes:\",\n",
    "                                   \">>To borrow from philosophy, you don't truly understand the color red\",\n",
    "                                   \">>until you have seen it.\"] # taken from /alt.atheism/51128\n",
    "#help(re.search)\n",
    "\n",
    "tokenized_toy_example = tokenize(toy_example_list_of_valid_lines, verbose=True)\n",
    "\n",
    "print(f\"\\nThe tokens in the toy example are: {', '.join(tokenized_toy_example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As mentioned above, there are a number of different ways to clean this sort of data. Another (possibly >= useful) method would be to tokenize by spaces and remove (entirely) tokens containing characters which are neither alphanumeric nor punctuation marks (specifically apostrophes and hyphens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_and_frequencies = Counter(tokenized_toy_example)\n",
    "tokens_and_frequencies.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was all done with the toy_example, let's now do it with the 20_newsgroup data\n",
    "\n",
    "\n",
    "### Finding the (token, frequency) tuples over the entire 20 newsgroup data set\n",
    "# NOTE: you don't need to run this if you have the *20_newsgroup_tokens_and_frequencies.pickle* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for topic_name, topic_dictionary in data_20_newsgroup.items():\\n    for filename, content_lines_in_filename in topic_dictionary.items():\\n        print(f\"iterating over {topic_name} , {filename}\")\\n        \\n        # content_lines_in_filename is a list of strings where each string is an untokenized line \\n        # (i.e. the string representation of the line in the file).\\n        \\n        # tokenizing the list\\n        tokenized_content_lines = tokenize(content_lines_in_filename) # returns a list\\n\\n        # adding the tokens to the \\n        for token in tokenized_content_lines: \\n            data_20_newsgroup_tokens.append(token)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '\\\\\\\\?\\\\' + os.path.abspath(os.path.join(os.getcwd(), '20_newsgroups'))\n",
    "subdirectory_list = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "with open('20_newsgroup_content_dictionary.pickle', 'rb') as handle:\n",
    "    data_20_newsgroup = pickle.load(handle)\n",
    "\n",
    "'''\n",
    "recall that the data_20_newsgroup dictionary is formatted as: \n",
    "\n",
    "data_20_newsgroup = {\n",
    "\n",
    "    'alt.atheism' : {\n",
    "\n",
    "                        '49960' : [ list of all the valid lines in file 49960 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                        '51060' : [ list of all the valid lines in file 51060 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                        ...\n",
    "\n",
    "                    },\n",
    "\n",
    "    'comp.graphics' : {\n",
    "\n",
    "                          '37261': [ list of all the valid lines in file 37261 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                          '37913': [ list of all the valid lines in file 37913 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                          ...\n",
    "\n",
    "                      } ,\n",
    "\n",
    "    ...\n",
    "\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "data_20_newsgroup_tokens = []\n",
    "\n",
    "'''for topic_name, topic_dictionary in data_20_newsgroup.items():\n",
    "    for filename, content_lines_in_filename in topic_dictionary.items():\n",
    "        print(f\"iterating over {topic_name} , {filename}\")\n",
    "        \n",
    "        # content_lines_in_filename is a list of strings where each string is an untokenized line \n",
    "        # (i.e. the string representation of the line in the file).\n",
    "        \n",
    "        # tokenizing the list\n",
    "        tokenized_content_lines = tokenize(content_lines_in_filename) # returns a list\n",
    "\n",
    "        # adding the tokens to the \n",
    "        for token in tokenized_content_lines: \n",
    "            data_20_newsgroup_tokens.append(token)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20_newsgroup_tokens_and_frequencies = Counter(data_20_newsgroup_tokens)\n",
    "with open('20_newsgroup_tokens_and_frequencies.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_20_newsgroup_tokens_and_frequencies, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256169\n"
     ]
    }
   ],
   "source": [
    "with open('!20_newsgroups_corpus_wide_tokens_and_frequencies_(lexicon).pickle', 'rb') as handle:\n",
    "    word_cross_corpus_freq_dict = pickle.load(handle)\n",
    "print(len(word_cross_corpus_freq_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating over alt.atheism , 49960\n"
     ]
    }
   ],
   "source": [
    "dict_of_dicts_docID_token_freq_dicts = {}\n",
    "'''\n",
    "dict_of_dicts_docID_token_freq_dicts = {\n",
    "    docID#1 = {\n",
    "        token#1:frequency in docID#1,\n",
    "        token#2:frequency in docID#1,\n",
    "        ...\n",
    "    },\n",
    "    docID#2 = {\n",
    "        token#1:frequency in docID#2,\n",
    "        token#2:frequency in docID#2,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "for topic_name, topic_dictionary in data_20_newsgroup.items(): # topic_name is one of 'alt.atheism', 'comp.graphics', ... and topic_dictionary is a filename::[list of valid lines in file] dictionary\n",
    "    for filename, content_lines_in_filename in topic_dictionary.items():\n",
    "        print(f\"iterating over {topic_name} , {filename}\")\n",
    "        this_file_tokens_and_frequencies_dict = {}\n",
    "        dict_of_dicts_docID_token_freq_dicts[filename] = this_file_tokens_and_frequencies_dict\n",
    "        for token in set(word_cross_corpus_freq_dict.keys()):\n",
    "            token_occurences_in_filename = 0\n",
    "            for line in content_lines_in_filename:\n",
    "                tokenized_list = tokenize(line)\n",
    "                token_occurences_in_filename += tokenized_list.count(token)\n",
    "            this_file_tokens_and_frequencies_dict[token] = token_occurences_in_filename\n",
    "        \n",
    "        dict_of_dicts_docID_token_Counter[filename] = token_occurences_in_filename\n",
    "        print(dict_of_dicts_docID_token_Counter)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the (token, frequency) computing code but in a topic-specific manner\n",
    "\n",
    "# DON'T NEED TO DO THIS IF YOU HAVE THE *20_newsgroup_tokens_and_frequencies_by_topics_dictionary.pickle* FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20_newsgroup_content_dictionary.pickle', 'rb') as handle:\n",
    "    data_20_newsgroup = pickle.load(handle)\n",
    "assert data_20_newsgroup == super_dictionary\n",
    "'''\n",
    "recall that the data_20_newsgroup dictionary is formatted as: \n",
    "\n",
    "data_20_newsgroup = {\n",
    "\n",
    "    'alt.atheism' : {\n",
    "\n",
    "                        '49960' : [ list of all the valid lines in file 49960 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                        '51060' : [ list of all the valid lines in file 51060 in the 'alt.atheism' subdirectory ],\n",
    "\n",
    "                        ...\n",
    "\n",
    "                    },\n",
    "\n",
    "    'comp.graphics' : {\n",
    "\n",
    "                          '37261': [ list of all the valid lines in file 37261 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                          '37913': [ list of all the valid lines in file 37913 in the 'comp.graphics' subdirectory ],\n",
    "\n",
    "                          ...\n",
    "\n",
    "                      } ,\n",
    "\n",
    "    ...\n",
    "\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "data_20_newsgroup_tokens_by_topics = {\n",
    "    'alt.atheism' : [],\n",
    "    'comp.graphics' : [],\n",
    "    'comp.os.ms-windows.misc' : [],\n",
    "    'comp.sys.ibm.pc.hardware' : [],\n",
    "    'comp.sys.mac.hardware' : [],\n",
    "    'comp.windows.x' : [],\n",
    "    'misc.forsale' : [],\n",
    "    'rec.autos' : [],\n",
    "    'rec.motorcycles' : [],\n",
    "    'rec.sport.baseball' : [],\n",
    "    'rec.sport.hockey' : [],\n",
    "    'sci.crypt' : [],\n",
    "    'sci.electronics' : [],\n",
    "    'sci.med' : [],\n",
    "    'sci.space' : [],\n",
    "    'soc.religion.christian' : [],\n",
    "    'talk.politics.guns' : [],\n",
    "    'talk.politics.mideast' : [],\n",
    "    'talk.politics.misc' : [],\n",
    "    'talk.religion.misc' : []\n",
    "}\n",
    "\n",
    "for topic_name, topic_dictionary in data_20_newsgroup.items():\n",
    "    print(f\"iterating over {topic_name}\")\n",
    "\n",
    "    tokens_in_this_topics_files = []\n",
    "    \n",
    "    for filename, content_lines_in_filename in topic_dictionary.items():\n",
    "        \n",
    "        # tokenizing the list\n",
    "        tokenized_content_lines = tokenize(content_lines_in_filename) # returns a list\n",
    "\n",
    "        # adding the tokens to the tokens_in_this_topics_files list\n",
    "        for token in tokenized_content_lines: \n",
    "            tokens_in_this_topics_files.append(token)\n",
    "        \n",
    "    this_topics_tokens_and_frequencies = Counter(tokens_in_this_topics_files)\n",
    "    this_topic_tokens_and_frequencies_as_list_of_tuples = [ (tok,freq) for (tok,freq) in this_topics_tokens_and_frequencies.items() ]\n",
    "\n",
    "    data_20_newsgroup_tokens_by_topics[topic_name] = this_topic_tokens_and_frequencies_as_list_of_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20_newsgroup_tokens_and_frequencies_by_topics_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_20_newsgroup_tokens_by_topics, handle)\n",
    "with open('20_newsgroup_tokens_and_frequencies_by_topics_dictionary.pickle', 'rb') as handle:\n",
    "    x = pickle.load(handle)\n",
    "print((x == data_20_newsgroup_tokens_by_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Storage - Industry Sector\n",
    "### This notebook assumes that you've downloaded the [industry sector](https://people.cs.umass.edu/~mccallum/data.html) and unpacked the tarball in the same directory as this notebook. \n",
    "\n",
    "### You should therefore have a folder called \"sector\" in the same directory as this notebook. The \"sector\" folder should have 12 subfolders (\"energy.sector\", \"capital.goods.sector\", etc...), each containing a long list of subfolders, themselves containing html-like documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Format - Industry Sector\n",
    "### Trying to find a good \"landmark line\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '\\\\\\\\?\\\\' + os.path.abspath(os.path.join(os.getcwd(), 'sector'))\n",
    "openable_file_count = 0\n",
    "file_count = 0\n",
    "landmark_count = 0\n",
    "strangely_encoded_files = []\n",
    "annoying_notebook_files_count = 0\n",
    "landmarkless_files = []\n",
    "\n",
    "for dirname, dirnames, filenames in os.walk(root_dir):\n",
    "    # print path to all subdirectories first.\n",
    "    for subdirname in dirnames:\n",
    "        #print(os.path.join(dirname, subdirname))\n",
    "        pass\n",
    "\n",
    "    # print path to all filenames.\n",
    "    annoying_notebook_files = [ x for x in filenames if '.onetoc2' in x ]\n",
    "    annoying_notebook_files_count += len(annoying_notebook_files)\n",
    "    for notebookfile in annoying_notebook_files:\n",
    "        filenames.remove(notebookfile)\n",
    "        \n",
    "    file_count += len(filenames)\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        with open(os.path.join(dirname, filename),'r', errors='ignore') as infile:\n",
    "            contents = infile.readlines()\n",
    "            found_landmark = False\n",
    "            for line in contents:\n",
    "                if 'content-type: text/html' in line.lower():\n",
    "                    landmark_count += 1\n",
    "                    found_landmark = True\n",
    "                    break\n",
    "            if not found_landmark: \n",
    "                landmarkless_files.append(dirname+filename)\n",
    "            openable_file_count += 1\n",
    "        \n",
    "\n",
    "print(f\"examined {openable_file_count}/{file_count} and counted {landmark_count} 'Content-type: text/html' lines\")\n",
    "print('found {} notebook files (ignoring them)'.format(str(annoying_notebook_files_count)))\n",
    "print('these {} files did not have the landmark:\\n\\n{}'.format( str(len(landmarkless_files)), '\\n\\n'.join(landmarkless_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like the line \"Content-type: text/html\" is a good landmark line, so now I'm going to initialize and populate the sector_super_dictionary.\n",
    "# NOTE: this dictionary has one level more than the previous super_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_subtopic_map = {\n",
    "    \"100\" : \"sector\\\\basic.materials.sector\\\\chemical.manufacturing.industry\",\n",
    "    \"101\" : \"sector\\\\basic.materials.sector\\\\chemicals.plastics.and.rubber.industry\",\n",
    "    \"102\" : \"sector\\\\basic.materials.sector\\\\containers.and.packaging.industry\",\n",
    "    \"103\" : \"sector\\\\basic.materials.sector\\\\fabricated.plastic.and.rubber.industry\",\n",
    "    \"104\" : \"sector\\\\basic.materials.sector\\\\forestry.and.wood.products.industry\",\n",
    "    \"105\" : \"sector\\\\basic.materials.sector\\\\gold.and.silver.industry\",\n",
    "    \"106\" : \"sector\\\\basic.materials.sector\\\\iron.and.steel.industry\",\n",
    "    \"107\" : \"sector\\\\basic.materials.sector\\\\metal.and.mining.industry\",\n",
    "    \"108\" : \"sector\\\\basic.materials.sector\\\\misc.fabricated.products.industry\",\n",
    "    \"109\" : \"sector\\\\basic.materials.sector\\\\paper.and.paper.products.industry\",\n",
    "    \"110\" : \"sector\\\\capital.goods.sector\\\\aerospace.and.defense.industry\",\n",
    "    \"111\" : \"sector\\\\capital.goods.sector\\\\construction.sector\\\\construction-raw.materials.industry\",\n",
    "    \"112\" : \"sector\\\\capital.goods.sector\\\\construction.sector\\\\construction-supplies.and.fixtures.industry\",\n",
    "    \"113\" : \"sector\\\\capital.goods.sector\\\\construction.sector\\\\construction.and.agricultural.machinery.industry\",\n",
    "    \"114\" : \"sector\\\\capital.goods.sector\\\\construction.sector\\\\construction.services.industry\",\n",
    "    \"115\" : \"sector\\\\capital.goods.sector\\\\misc.capital.goods.industry\",\n",
    "    \"116\" : \"sector\\\\capital.goods.sector\\\\mobile.homes.and.rvs.industry\",\n",
    "    \"117\" : \"sector\\\\conglomerates.industry\",\n",
    "    \"118\" : \"sector\\\\consumer.cyclical.sector\\\\appliance.and.tool.industry\",\n",
    "    \"119\" : \"sector\\\\consumer.cyclical.sector\\\\audio.and.video.equipment.industry\",\n",
    "    \"120\" : \"sector\\\\consumer.cyclical.sector\\\\auto.sector\\\\auto.and.truck.manufacturers.industry\",\n",
    "    \"121\" : \"sector\\\\consumer.cyclical.sector\\\\auto.sector\\\\auto.and.truck.parts.industry\",\n",
    "    \"122\" : \"sector\\\\consumer.cyclical.sector\\\\footwear.industry\",\n",
    "    \"123\" : \"sector\\\\consumer.cyclical.sector\\\\furniture.and.fixtures.industry\",\n",
    "    \"124\" : \"sector\\\\consumer.cyclical.sector\\\\jewelry.and.silverware.industry\",\n",
    "    \"125\" : \"sector\\\\consumer.cyclical.sector\\\\photography.industry\",\n",
    "    \"126\" : \"sector\\\\consumer.cyclical.sector\\\\recreational.products.industry\",\n",
    "    \"127\" : \"sector\\\\consumer.cyclical.sector\\\\textiles.non.apparel.industry\",\n",
    "    \"128\" : \"sector\\\\consumer.cyclical.sector\\\\tires.industry\",\n",
    "    \"129\" : \"sector\\\\consumer.non-cyclical.sector\\\\beverages.sector\\\\alcoholic.beverages.industry\",\n",
    "    \"130\" : \"sector\\\\consumer.non-cyclical.sector\\\\beverages.sector\\\\non.alcoholic.beverages.industry\",\n",
    "    \"131\" : \"sector\\\\consumer.non-cyclical.sector\\\\crops.industry\",\n",
    "    \"132\" : \"sector\\\\consumer.non-cyclical.sector\\\\fish.livestock.industry\",\n",
    "    \"133\" : \"sector\\\\consumer.non-cyclical.sector\\\\food.processing.industry\",\n",
    "    \"134\" : \"sector\\\\consumer.non-cyclical.sector\\\\office.supplies.industry\",\n",
    "    \"135\" : \"sector\\\\consumer.non-cyclical.sector\\\\personal.and.household.products.industry\",\n",
    "    \"136\" : \"sector\\\\consumer.non-cyclical.sector\\\\tobacco.industry\",\n",
    "    \"137\" : \"sector\\\\energy.sector\\\\coal.industry\",\n",
    "    \"138\" : \"sector\\\\energy.sector\\\\oil.and.gas.integrated.industry\",\n",
    "    \"139\" : \"sector\\\\energy.sector\\\\oil.and.gas.operations.industry\",\n",
    "    \"140\" : \"sector\\\\energy.sector\\\\oil.well.services.and.equipment.industry\",\n",
    "    \"141\" : \"sector\\\\financial.sector\\\\banking.sector\\\\money.center.banks.industry\",\n",
    "    \"142\" : \"sector\\\\financial.sector\\\\banking.sector\\\\regional.banks.industry\",\n",
    "    \"143\" : \"sector\\\\financial.sector\\\\banking.sector\\\\s.and.ls.savings.banks.industry\",\n",
    "    \"144\" : \"sector\\\\financial.sector\\\\consumer.financial.services.industry\",\n",
    "    \"145\" : \"sector\\\\financial.sector\\\\insurance.sector\\\\accident.and.health.insurance.industry\",\n",
    "    \"146\" : \"sector\\\\financial.sector\\\\insurance.sector\\\\life.insurance.industry\",\n",
    "    \"147\" : \"sector\\\\financial.sector\\\\insurance.sector\\\\misc.insurance.industry\",\n",
    "    \"148\" : \"sector\\\\financial.sector\\\\insurance.sector\\\\property.and.casualty.insurance.industry\",\n",
    "    \"149\" : \"sector\\\\financial.sector\\\\investment.services.industry\",\n",
    "    \"150\" : \"sector\\\\financial.sector\\\\misc.financial.services.industry\",\n",
    "    \"151\" : \"sector\\\\healthcare.sector\\\\biotechnology.and.drugs.industry\",\n",
    "    \"152\" : \"sector\\\\healthcare.sector\\\\healthcare.facilities.industry\",\n",
    "    \"153\" : \"sector\\\\healthcare.sector\\\\major.drugs.industry\",\n",
    "    \"154\" : \"sector\\\\healthcare.sector\\\\medical.equipment.and.supplies.industry\",\n",
    "    \"155\" : \"sector\\\\services.sector\\\\advertising.industry\",\n",
    "    \"156\" : \"sector\\\\services.sector\\\\broadcasting.and.cable.tv.industry\",\n",
    "    \"157\" : \"sector\\\\services.sector\\\\business.services.industry\",\n",
    "    \"158\" : \"sector\\\\services.sector\\\\casinos.and.gambling.industry\",\n",
    "    \"159\" : \"sector\\\\services.sector\\\\communications.services.industry\",\n",
    "    \"160\" : \"sector\\\\services.sector\\\\hotels.and.motels.industry\",\n",
    "    \"161\" : \"sector\\\\services.sector\\\\law.sector\\\\immigration.law.industry\",\n",
    "    \"162\" : \"sector\\\\services.sector\\\\law.sector\\\\international.law.industry\",\n",
    "    \"163\" : \"sector\\\\services.sector\\\\law.sector\\\\maritime.law.industry\",\n",
    "    \"164\" : \"sector\\\\services.sector\\\\law.sector\\\\trade.law.industry\",\n",
    "    \"165\" : \"sector\\\\services.sector\\\\motion.pictures.industry\",\n",
    "    \"166\" : \"sector\\\\services.sector\\\\personal.services.industry\",\n",
    "    \"167\" : \"sector\\\\services.sector\\\\printing.and.publishing.industry\",\n",
    "    \"168\" : \"sector\\\\services.sector\\\\printing.services.industry\",\n",
    "    \"169\" : \"sector\\\\services.sector\\\\real.estate.operations.industry\",\n",
    "    \"170\" : \"sector\\\\services.sector\\\\recreational.activities.industry\",\n",
    "    \"171\" : \"sector\\\\services.sector\\\\rental.and.leasing.industry\",\n",
    "    \"172\" : \"sector\\\\services.sector\\\\restaurants.industry\",\n",
    "    \"173\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.apparel.industry\",\n",
    "    \"174\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.catalog.and.mail.order.industry\",\n",
    "    \"175\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.department.and.discount.industry\",\n",
    "    \"176\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.drugs.industry\",\n",
    "    \"177\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.grocery.industry\",\n",
    "    \"178\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.home.improvement.industry\",\n",
    "    \"179\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.specialty.industry\",\n",
    "    \"180\" : \"sector\\\\services.sector\\\\retail.sector\\\\retail.technology.industry\",\n",
    "    \"181\" : \"sector\\\\services.sector\\\\schools.industry\",\n",
    "    \"182\" : \"sector\\\\services.sector\\\\security.systems.and.services.industry\",\n",
    "    \"183\" : \"sector\\\\services.sector\\\\waste.management.services.industry\",\n",
    "    \"184\" : \"sector\\\\technology.sector\\\\communications.equipment.industry\",\n",
    "    \"185\" : \"sector\\\\technology.sector\\\\computer.sector\\\\computer.hardware.industry\",\n",
    "    \"186\" : \"sector\\\\technology.sector\\\\computer.sector\\\\computer.networks.industry\",\n",
    "    \"187\" : \"sector\\\\technology.sector\\\\computer.sector\\\\computer.peripherals.industry\",\n",
    "    \"188\" : \"sector\\\\technology.sector\\\\computer.sector\\\\computer.services.industry\",\n",
    "    \"189\" : \"sector\\\\technology.sector\\\\computer.sector\\\\computer.storage.devices.industry\",\n",
    "    \"190\" : \"sector\\\\technology.sector\\\\computer.sector\\\\software.and.programming.industry\",\n",
    "    \"191\" : \"sector\\\\technology.sector\\\\electronic.instruments.and.controls.industry\",\n",
    "    \"192\" : \"sector\\\\technology.sector\\\\office.equipment.industry\",\n",
    "    \"193\" : \"sector\\\\technology.sector\\\\scientific.and.technical.instruments.industry\",\n",
    "    \"194\" : \"sector\\\\technology.sector\\\\semiconductors.industry\",\n",
    "    \"195\" : \"sector\\\\transportation.sector\\\\air.courier.industry\",\n",
    "    \"196\" : \"sector\\\\transportation.sector\\\\airline.industry\",\n",
    "    \"197\" : \"sector\\\\transportation.sector\\\\misc.transportation.industry\",\n",
    "    \"198\" : \"sector\\\\transportation.sector\\\\railroad.industry\",\n",
    "    \"199\" : \"sector\\\\transportation.sector\\\\trucking.industry\",\n",
    "    \"200\" : \"sector\\\\transportation.sector\\\\water.transportation.industry\",\n",
    "    \"201\" : \"sector\\\\utilities.sector\\\\electric.utilities.industry\",\n",
    "    \"202\" : \"sector\\\\utilities.sector\\\\natural.gas.industry\",\n",
    "    \"203\" : \"sector\\\\utilities.sector\\\\water.utilities.industry\"\n",
    "}\n",
    "\n",
    "data_industry_sector_tokens_by_topics = {}\n",
    "for key in key_subtopic_map.keys():\n",
    "    data_industry_sector_tokens_by_topics[key] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating sector_super_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '\\\\\\\\?\\\\' + os.path.abspath(os.path.join(os.getcwd()))\n",
    "file_count = 0\n",
    "all_tokens_in_data_set = []\n",
    "\n",
    "for key, subtopic_path in key_subtopic_map.items():\n",
    "    tokens_in_this_subtopics_files = []\n",
    "    for file in os.listdir(os.path.abspath(os.path.join(root_dir, subtopic_path ))):\n",
    "        lines_with_valid_content = []\n",
    "        with open(os.path.abspath(os.path.join(root_dir, subtopic_path, file )), 'r', errors='ignore') as inputfile:\n",
    "            contents = inputfile.readlines()\n",
    "        for i, line in enumerate(contents):\n",
    "            if \"content-type: text/html\" in line.lower():\n",
    "                lines_with_valid_content.append(line.rstrip())\n",
    "                break\n",
    "        i += 1\n",
    "        while i < len(contents):\n",
    "            lines_with_valid_content.append(contents[i].rstrip())\n",
    "            i += 1\n",
    "        tokenized_content_lines = tokenize(lines_with_valid_content) # returns a list\n",
    "\n",
    "        # adding the tokens to the tokens_in_this_topics_files list\n",
    "        for token in tokenized_content_lines: \n",
    "            tokens_in_this_subtopics_files.append(token)\n",
    "            all_tokens_in_data_set.append(token)\n",
    "        \n",
    "    this_subtopics_tokens_and_frequencies = Counter(tokens_in_this_subtopics_files)\n",
    "    this_subtopic_tokens_and_frequencies_as_list_of_tuples = [ (tok,freq) for (tok,freq) in this_subtopics_tokens_and_frequencies.items() ]\n",
    "\n",
    "    data_industry_sector_tokens_by_topics[key] = this_subtopic_tokens_and_frequencies_as_list_of_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_industry_sector_tokens_by_topics[\"100\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tokens_and_token_frequencies_in_data_set = Counter(all_tokens_in_data_set)\n",
    "all_tokens_and_token_frequencies_in_data_set_as_list_of_tuples = [ (tok,freq) for (tok,freq) in all_tokens_and_token_frequencies_in_data_set.items() ]\n",
    "print(len(all_tokens_and_token_frequencies_in_data_set_as_list_of_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(data_industry_sector_tokens_by_topics))\n",
    "for i,x in enumerate(data_industry_sector_tokens_by_topics.keys()):\n",
    "    \n",
    "    print(len(data_industry_sector_tokens_by_topics[x]))\n",
    "    if i > 0:\n",
    "        print(data_industry_sector_tokens_by_topics[x] == data_industry_sector_tokens_by_topics[list(data_industry_sector_tokens_by_topics.keys())[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_industry_sector_tokens_by_topics, key_subtopic_map\n",
    "with open('industry_sector_tokens_and_frequencies_by_topics_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_industry_sector_tokens_by_topics, handle)\n",
    "with open('industry_sector_key_subtopic_mapping_dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(key_subtopic_map, handle)\n",
    "with open('industry_sector_tokens_and_frequencies_across_dataset_list_of_tuples.pickle','wb') as handle:\n",
    "    pickle.dump(all_tokens_and_token_frequencies_in_data_set_as_list_of_tuples, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
